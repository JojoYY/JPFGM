{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "This notebook\n",
    "- preprocesses all the various data sources, including cleaning and feature engineering\n",
    "- combines all the data sources\n",
    "- trains and tunes a lightGBM model\n",
    "\n",
    "To run on your local machine, please set `path_to_kaggle_data` below to the folder with all the raw data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the user: specify location of the unzipped data files on local machine\n",
    "path_to_kaggle_data='/home/johanna/kaggle_JPFGM/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "Overview of data sources\n",
    "- **application_{train|test}.csv (307511 x 122)** :  Static data for all applications. Each row corresponds to one loan application (not necessarily one person!).\n",
    "    - type of loan (e.g. contract_type, credit amount, annuity). *probably important*\n",
    "    - external credit scores *probably important*\n",
    "    - other loan application information (e.g. day of week, time of day). *probably not important*\n",
    "    - applicant income (e.g. income_total, income_type).  *probably important*\n",
    "    - employment history (days_employed, ID change, occupation, organization typer). *probably important*\n",
    "    - demographic information (gender, education, family status and members, housing situation, region of residence and rating, age)  *probably important*\n",
    "    - other ownership information (car, home, phone).  *probably important*\n",
    "    - lots of details about buiding of residence. *likely lots of redundant information and missing values*\n",
    "    - verification info (e.g. provided email, contact address match, documents provided)  *probably important, for fraud*\n",
    "    - number of enquiries to credit bureau  *probably important, as a measure of desperation*\n",
    "    - social circle default  *probably important*\n",
    "- **bureau.csv (x 15)**: previous credits provided by other financial institutions according to Credit Bureau. Each row corresponds to a previous credit\n",
    "    - timeline of other credits  *probably important for gauging other financial oblications*\n",
    "    - type and amount of other credits  *probably important for gauging other financial oblications*\n",
    "    - repayment status of other credits  *probably important for gauging trustworthiness/ability to repay*\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(path_to_kaggle_data, num_rows = None, nan_as_category = False):\n",
    "    \"\"\"Preprocess application_train.csv and application_test.csv.\n",
    "    \n",
    "    Remove ineligible observations, encode categorical features, replace outliers and\n",
    "    create new features.\n",
    "    \"\"\"\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv(path_to_kaggle_data + '/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv(path_to_kaggle_data + '/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(path_to_kaggle_data, num_rows = None, nan_as_category = True):\n",
    "    \"\"\"Preprocess bureau.csv and bureau_balance.csv.\n",
    "    \n",
    "    Encode categorical features, aggregates bureau_balance data\n",
    "    \"\"\"\n",
    "    bureau = pd.read_csv(path_to_kaggle_data + '/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv(path_to_kaggle_data + '/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    \n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    \n",
    "    # bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')  # generates error\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left')  # fix: dataframe.join uses index, do not specify column nane\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    # bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')  # generates error\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left')  # fix\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(path_to_kaggle_data, num_rows = None, nan_as_category = True):\n",
    "    \"\"\"Preprocess previous_applications.csv.\n",
    "    \n",
    "    Impute missing data and aggregate previous_applications.csv\n",
    "    \"\"\"\n",
    "    prev = pd.read_csv(path_to_kaggle_data + '/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    \n",
    "    # prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR') # error\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left')  # fix\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    # prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')  # error\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(path_to_kaggle_data, num_rows = None, nan_as_category = True):\n",
    "    \"\"\"Preprocess POS_CASH_balance.csv.\n",
    "    \n",
    "    Encodes categorical features and aggregates POS_CASH_balance.csv\n",
    "    \"\"\"\n",
    "    pos = pd.read_csv(path_to_kaggle_data + '/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(path_to_kaggle_data, num_rows = None, nan_as_category = True):\n",
    "    \"\"\"Preprocess installments_payments.csv.\n",
    "    \n",
    "    Encodes categorical features, creates new features and aggregates installments_payments.csv\n",
    "    \"\"\"\n",
    "    ins = pd.read_csv(path_to_kaggle_data + '/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(path_to_kaggle_data, num_rows = None, nan_as_category = True):\n",
    "    \"\"\"Preprocess credit_card_balance.csv.\n",
    "    \n",
    "    Encodes categorical features and aggregates credit_card_balance.csv\n",
    "    \"\"\"\n",
    "    cc = pd.read_csv(path_to_kaggle_data + '/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_prepare(debug = False):\n",
    "    \"\"\"Pre-process all the separate data sets and join them.\"\"\"\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(path_to_kaggle_data, num_rows)\n",
    "    \n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(path_to_kaggle_data, num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(path_to_kaggle_data, num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(path_to_kaggle_data, num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(path_to_kaggle_data, num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(path_to_kaggle_data, num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 307511, test samples: 48744\n",
      "Bureau df shape: (305811, 116)\n",
      "Process bureau and bureau_balance - done in 17s\n",
      "Previous applications df shape: (338857, 249)\n",
      "Process previous_applications - done in 20s\n",
      "Pos-cash balance df shape: (337252, 18)\n",
      "Process POS-CASH balance - done in 12s\n",
      "Installments payments df shape: (339587, 26)\n",
      "Process installments payments - done in 32s\n",
      "Credit card balance df shape: (103558, 141)\n",
      "Process credit card balance - done in 16s\n"
     ]
    }
   ],
   "source": [
    "# Features generated by above functions\n",
    "base_features = feature_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 798)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original ipynb uses sqlite, but it's easier here to use pandas directly.\n",
    "pay = pd.read_csv(path_to_kaggle_data + '/user_behaviour_pay_status.csv')\n",
    "full = base_features.set_index('SK_ID_CURR').join(pay.set_index('SK_ID_CURR'), how='left')\n",
    "del pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 899)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lightgbm(train_df, test_df, submission_file_name, num_folds, stratified = False, debug= False):\n",
    "    \"\"\"LightGBM GBDT with KFold or Stratified KFold.\n",
    "    \n",
    "    Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "    \"\"\"\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 100, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_df, test_df, submission_file_name, debug=False):\n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        feat_importance = kfold_lightgbm(train_df, test_df, submission_file_name, num_folds= 5, stratified= False, debug= debug)\n",
    "\n",
    "def main_10(train_df, test_df, submission_file_name, debug=False):\n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        feat_importance = kfold_lightgbm(train_df, test_df, submission_file_name, num_folds= 10, stratified= False, debug= debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((282682, 899), (24825, 899))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "goods = full.query('TARGET == 0')\n",
    "defaults = full.query('TARGET == 1')\n",
    "goods.shape,defaults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 28.83\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "variables = np.array([])\n",
    "scores = np.array([])\n",
    "for v in full.columns:\n",
    "    variable = v\n",
    "    ks = stats.ks_2samp(goods[v], defaults[v])\n",
    "    variables = np.hstack((variables,v))\n",
    "    scores = np.hstack((scores,ks[0]))\n",
    "end = time.time() \n",
    "print('time elapsed: {:.2f}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.22328885, 0.19746255, 0.15466213, 0.13726573,\n",
       "       0.13550183, 0.13329567, 0.13089324, 0.12562523, 0.12437477,\n",
       "       0.12388313, 0.12243642, 0.12187878, 0.1202876 , 0.11703069,\n",
       "       0.1145477 , 0.11443279, 0.11051882, 0.10937092, 0.10707338,\n",
       "       0.10544895, 0.10471876, 0.1039575 , 0.10356142, 0.1033065 ,\n",
       "       0.1031189 , 0.10288956, 0.10276447, 0.10215221, 0.1009105 ,\n",
       "       0.10059598, 0.10054135, 0.10039111, 0.10015082, 0.10010902,\n",
       "       0.09960581, 0.09901066, 0.09900438, 0.098952  , 0.09894492,\n",
       "       0.09884473, 0.09883936, 0.09778391, 0.09715205, 0.09702436,\n",
       "       0.09692013, 0.09680171, 0.09624227, 0.0956651 , 0.09524613,\n",
       "       0.09481291, 0.09458678, 0.09451663, 0.09419523, 0.09199233,\n",
       "       0.09184592, 0.09183344, 0.09160366, 0.09157524, 0.09152345,\n",
       "       0.09139779, 0.09070291, 0.09019879, 0.09016059, 0.08975165,\n",
       "       0.0897375 , 0.08972469, 0.08915517, 0.0883045 , 0.08719978,\n",
       "       0.08673038, 0.08657749, 0.08648037, 0.08559959, 0.08527263,\n",
       "       0.085212  , 0.08518795, 0.08385014, 0.08368867, 0.08365467,\n",
       "       0.08355801, 0.08355801, 0.08355801, 0.08355801, 0.08301649,\n",
       "       0.08297158, 0.08275996, 0.08270474, 0.08219022, 0.08215131,\n",
       "       0.08201028, 0.08198699, 0.08078992, 0.0803808 , 0.08036809,\n",
       "       0.08025252, 0.08010706, 0.07984191, 0.07979544, 0.07975653])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[np.argsort(-scores)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TARGET', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'BURO_DAYS_CREDIT_MEAN',\n",
       "       'BURO_DAYS_CREDIT_UPDATE_MEAN', 'DAYS_EMPLOYED_PERC',\n",
       "       'BURO_CREDIT_ACTIVE_Active_MEAN', 'CLOSED_DAYS_CREDIT_MEAN',\n",
       "       'BURO_DAYS_CREDIT_MAX', 'CLOSED_DAYS_CREDIT_MIN',\n",
       "       'BURO_DAYS_CREDIT_MIN', 'num_earlypay_1000_normalized',\n",
       "       'DAYS_BIRTH', 'BURO_DAYS_CREDIT_ENDDATE_MEAN',\n",
       "       'num_earlypay_600_normalized', 'CLOSED_DAYS_CREDIT_UPDATE_MEAN',\n",
       "       'CLOSED_DAYS_CREDIT_ENDDATE_MIN', 'BURO_DAYS_CREDIT_ENDDATE_MIN',\n",
       "       'num_latepay_1000_normalized', 'num_latepay_1000',\n",
       "       'NAME_INCOME_TYPE_Working',\n",
       "       'PREV_NAME_CONTRACT_STATUS_Approved_MEAN',\n",
       "       'CLOSED_DAYS_CREDIT_MAX', 'PREV_CODE_REJECT_REASON_XAP_MEAN',\n",
       "       'num_earlypay_all_normalized', 'CLOSED_DAYS_CREDIT_ENDDATE_MEAN',\n",
       "       'num_earlypay_300_normalized', 'num_latepay_600_normalized',\n",
       "       'num_latepay_600', 'REFUSED_AMT_APPLICATION_MIN',\n",
       "       'REFUSED_AMT_CREDIT_MIN', 'REFUSED_AMT_APPLICATION_MEAN',\n",
       "       'REFUSED_HOUR_APPR_PROCESS_START_MIN', 'REFUSED_AMT_CREDIT_MEAN',\n",
       "       'REFUSED_HOUR_APPR_PROCESS_START_MEAN',\n",
       "       'BURO_AMT_CREDIT_SUM_DEBT_MEAN', 'REFUSED_AMT_APPLICATION_MAX',\n",
       "       'REFUSED_AMT_CREDIT_MAX', 'REFUSED_DAYS_DECISION_MEAN',\n",
       "       'REFUSED_DAYS_DECISION_MIN', 'REFUSED_HOUR_APPR_PROCESS_START_MAX',\n",
       "       'REFUSED_DAYS_DECISION_MAX', 'PREV_APP_CREDIT_PERC_MEAN',\n",
       "       'INSTAL_AMT_PAYMENT_MIN', 'REFUSED_APP_CREDIT_PERC_MIN',\n",
       "       'REFUSED_APP_CREDIT_PERC_MEAN', 'DAYS_LAST_PHONE_CHANGE',\n",
       "       'num_latepay_all_normalized', 'BURO_CREDIT_ACTIVE_Closed_MEAN',\n",
       "       'CODE_GENDER', 'APPROVED_APP_CREDIT_PERC_MAX', 'PAYMENT_RATE',\n",
       "       'INSTAL_PAYMENT_PERC_MEAN', 'AMT_GOODS_PRICE',\n",
       "       'REFUSED_APP_CREDIT_PERC_MAX', 'REFUSED_AMT_GOODS_PRICE_MIN',\n",
       "       'PREV_APP_CREDIT_PERC_MAX', 'REFUSED_AMT_ANNUITY_MIN',\n",
       "       'REFUSED_AMT_ANNUITY_MEAN', 'REFUSED_AMT_ANNUITY_MAX',\n",
       "       'REFUSED_AMT_GOODS_PRICE_MEAN', 'num_earlypay_all',\n",
       "       'CLOSED_DAYS_CREDIT_VAR', 'REFUSED_AMT_GOODS_PRICE_MAX',\n",
       "       'REFUSED_CNT_PAYMENT_SUM', 'REFUSED_CNT_PAYMENT_MEAN',\n",
       "       'PREV_NAME_CONTRACT_STATUS_Refused_MEAN',\n",
       "       'NAME_EDUCATION_TYPE_Higher education',\n",
       "       'num_earlypay_100_normalized', 'INSTAL_DPD_MEAN', 'EXT_SOURCE_1',\n",
       "       'DAYS_EMPLOYED', 'PREV_NAME_YIELD_GROUP_low_normal_MEAN',\n",
       "       'BURO_DAYS_CREDIT_ENDDATE_MAX', 'CLOSED_DAYS_CREDIT_ENDDATE_MAX',\n",
       "       'DAYS_ID_PUBLISH', 'PREV_APP_CREDIT_PERC_MIN',\n",
       "       'CLOSED_AMT_CREDIT_SUM_OVERDUE_MEAN',\n",
       "       'CLOSED_CREDIT_DAY_OVERDUE_MEAN', 'CLOSED_CREDIT_DAY_OVERDUE_MAX',\n",
       "       'CLOSED_AMT_CREDIT_SUM_SUM', 'CLOSED_AMT_CREDIT_SUM_MEAN',\n",
       "       'CLOSED_CNT_CREDIT_PROLONG_SUM', 'CLOSED_AMT_CREDIT_SUM_MAX',\n",
       "       'BURO_AMT_CREDIT_SUM_DEBT_SUM',\n",
       "       'NAME_EDUCATION_TYPE_Secondary / secondary special',\n",
       "       'CLOSED_AMT_CREDIT_SUM_LIMIT_SUM',\n",
       "       'CLOSED_AMT_CREDIT_SUM_LIMIT_MEAN',\n",
       "       'CLOSED_AMT_CREDIT_SUM_DEBT_MAX', 'CLOSED_AMT_CREDIT_SUM_DEBT_SUM',\n",
       "       'CLOSED_AMT_CREDIT_SUM_DEBT_MEAN', 'ACTIVE_DAYS_CREDIT_MAX',\n",
       "       'BURO_CREDIT_TYPE_Credit card_MEAN',\n",
       "       'APPROVED_AMT_DOWN_PAYMENT_MAX', 'BURO_AMT_CREDIT_SUM_DEBT_MAX',\n",
       "       'PREV_DAYS_DECISION_MEAN', 'APPROVED_APP_CREDIT_PERC_MEAN',\n",
       "       'INSTAL_DAYS_ENTRY_PAYMENT_MEAN', 'num_latepay_300_normalized',\n",
       "       'num_latepay_300'], dtype='<U69')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables[np.argsort(-scores)][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM with cross validation and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lightgbm(full, num_features, file_name):\n",
    "    top_num = num_features\n",
    "    feature_set_train = [v for v in variables[np.argsort(-scores)][0:top_num] if v != 'SK_ID_CURR'] # 0 is target\n",
    "    feature_set_test = [v for v in variables[np.argsort(-scores)][1:top_num] if v != 'SK_ID_CURR'] \n",
    "\n",
    "    # make 'SK_ID_CURR' a feature again\n",
    "    # sub_train = full.query('TARGET.notna()')[feature_set_train].reset_index() ## may work with updated pandas\n",
    "    sub_train = full.copy().loc[full['TARGET'].notnull(), feature_set_train].reset_index()\n",
    "    sub_test = full.copy().loc[full['TARGET'].isnull(), feature_set_test].reset_index()\n",
    "                         \n",
    "    with timer(\"Full model run\"):\n",
    "        main(sub_train, sub_test, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (307507, 101), test shape: (48744, 100)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.771581\tvalid_1's auc: 0.760373\n",
      "[200]\ttraining's auc: 0.789118\tvalid_1's auc: 0.771456\n",
      "[300]\ttraining's auc: 0.799572\tvalid_1's auc: 0.775901\n",
      "[400]\ttraining's auc: 0.807625\tvalid_1's auc: 0.778457\n",
      "[500]\ttraining's auc: 0.814182\tvalid_1's auc: 0.779815\n",
      "[600]\ttraining's auc: 0.819947\tvalid_1's auc: 0.78054\n",
      "[700]\ttraining's auc: 0.825387\tvalid_1's auc: 0.781005\n",
      "[800]\ttraining's auc: 0.830508\tvalid_1's auc: 0.781241\n",
      "[900]\ttraining's auc: 0.835315\tvalid_1's auc: 0.781506\n",
      "[1000]\ttraining's auc: 0.839706\tvalid_1's auc: 0.781787\n",
      "[1100]\ttraining's auc: 0.843998\tvalid_1's auc: 0.78192\n",
      "[1200]\ttraining's auc: 0.848267\tvalid_1's auc: 0.782168\n",
      "[1300]\ttraining's auc: 0.852349\tvalid_1's auc: 0.782261\n",
      "[1400]\ttraining's auc: 0.856281\tvalid_1's auc: 0.782366\n",
      "[1500]\ttraining's auc: 0.860094\tvalid_1's auc: 0.78235\n",
      "[1600]\ttraining's auc: 0.863458\tvalid_1's auc: 0.782414\n",
      "[1700]\ttraining's auc: 0.867133\tvalid_1's auc: 0.782255\n",
      "Early stopping, best iteration is:\n",
      "[1541]\ttraining's auc: 0.861545\tvalid_1's auc: 0.782449\n",
      "Fold  1 AUC : 0.782449\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.772142\tvalid_1's auc: 0.755358\n",
      "[200]\ttraining's auc: 0.789536\tvalid_1's auc: 0.766569\n",
      "[300]\ttraining's auc: 0.799985\tvalid_1's auc: 0.771169\n",
      "[400]\ttraining's auc: 0.808344\tvalid_1's auc: 0.774016\n",
      "[500]\ttraining's auc: 0.815075\tvalid_1's auc: 0.775498\n",
      "[600]\ttraining's auc: 0.821038\tvalid_1's auc: 0.776201\n",
      "[700]\ttraining's auc: 0.826167\tvalid_1's auc: 0.776669\n",
      "[800]\ttraining's auc: 0.831125\tvalid_1's auc: 0.777336\n",
      "[900]\ttraining's auc: 0.835911\tvalid_1's auc: 0.777636\n",
      "[1000]\ttraining's auc: 0.840526\tvalid_1's auc: 0.777772\n",
      "[1100]\ttraining's auc: 0.844936\tvalid_1's auc: 0.777726\n",
      "[1200]\ttraining's auc: 0.849101\tvalid_1's auc: 0.777723\n",
      "Early stopping, best iteration is:\n",
      "[1043]\ttraining's auc: 0.842446\tvalid_1's auc: 0.777842\n",
      "Fold  2 AUC : 0.777842\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.772123\tvalid_1's auc: 0.754945\n",
      "[200]\ttraining's auc: 0.789868\tvalid_1's auc: 0.766631\n",
      "[300]\ttraining's auc: 0.800233\tvalid_1's auc: 0.771747\n",
      "[400]\ttraining's auc: 0.808083\tvalid_1's auc: 0.774205\n",
      "[500]\ttraining's auc: 0.815003\tvalid_1's auc: 0.775732\n",
      "[600]\ttraining's auc: 0.821299\tvalid_1's auc: 0.776951\n",
      "[700]\ttraining's auc: 0.826748\tvalid_1's auc: 0.777284\n",
      "[800]\ttraining's auc: 0.8319\tvalid_1's auc: 0.777785\n",
      "[900]\ttraining's auc: 0.836828\tvalid_1's auc: 0.778217\n",
      "[1000]\ttraining's auc: 0.841261\tvalid_1's auc: 0.778559\n",
      "[1100]\ttraining's auc: 0.845627\tvalid_1's auc: 0.778623\n",
      "[1200]\ttraining's auc: 0.849966\tvalid_1's auc: 0.778814\n",
      "[1300]\ttraining's auc: 0.853941\tvalid_1's auc: 0.778783\n",
      "[1400]\ttraining's auc: 0.857746\tvalid_1's auc: 0.778696\n",
      "Early stopping, best iteration is:\n",
      "[1251]\ttraining's auc: 0.851978\tvalid_1's auc: 0.778884\n",
      "Fold  3 AUC : 0.778884\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.772306\tvalid_1's auc: 0.754888\n",
      "[200]\ttraining's auc: 0.790053\tvalid_1's auc: 0.766196\n",
      "[300]\ttraining's auc: 0.800473\tvalid_1's auc: 0.770818\n",
      "[400]\ttraining's auc: 0.808107\tvalid_1's auc: 0.773508\n",
      "[500]\ttraining's auc: 0.814625\tvalid_1's auc: 0.77488\n",
      "[600]\ttraining's auc: 0.820633\tvalid_1's auc: 0.775921\n",
      "[700]\ttraining's auc: 0.825924\tvalid_1's auc: 0.776522\n",
      "[800]\ttraining's auc: 0.831085\tvalid_1's auc: 0.776587\n",
      "[900]\ttraining's auc: 0.835829\tvalid_1's auc: 0.776752\n",
      "[1000]\ttraining's auc: 0.840492\tvalid_1's auc: 0.777095\n",
      "[1100]\ttraining's auc: 0.845133\tvalid_1's auc: 0.777249\n",
      "[1200]\ttraining's auc: 0.849052\tvalid_1's auc: 0.777308\n",
      "[1300]\ttraining's auc: 0.85298\tvalid_1's auc: 0.777278\n",
      "[1400]\ttraining's auc: 0.856655\tvalid_1's auc: 0.777251\n",
      "Early stopping, best iteration is:\n",
      "[1248]\ttraining's auc: 0.850986\tvalid_1's auc: 0.777356\n",
      "Fold  4 AUC : 0.777356\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.771152\tvalid_1's auc: 0.757345\n",
      "[200]\ttraining's auc: 0.788706\tvalid_1's auc: 0.770169\n",
      "[300]\ttraining's auc: 0.79947\tvalid_1's auc: 0.775219\n",
      "[400]\ttraining's auc: 0.807899\tvalid_1's auc: 0.777905\n",
      "[500]\ttraining's auc: 0.814548\tvalid_1's auc: 0.77896\n",
      "[600]\ttraining's auc: 0.82055\tvalid_1's auc: 0.77956\n",
      "[700]\ttraining's auc: 0.825822\tvalid_1's auc: 0.780016\n",
      "[800]\ttraining's auc: 0.831018\tvalid_1's auc: 0.780183\n",
      "[900]\ttraining's auc: 0.836133\tvalid_1's auc: 0.780582\n",
      "[1000]\ttraining's auc: 0.840675\tvalid_1's auc: 0.780857\n",
      "[1100]\ttraining's auc: 0.845067\tvalid_1's auc: 0.780905\n",
      "[1200]\ttraining's auc: 0.849115\tvalid_1's auc: 0.78078\n",
      "[1300]\ttraining's auc: 0.853025\tvalid_1's auc: 0.780653\n",
      "Early stopping, best iteration is:\n",
      "[1114]\ttraining's auc: 0.845675\tvalid_1's auc: 0.78095\n",
      "Fold  5 AUC : 0.780950\n",
      "Full AUC score 0.779476\n",
      "Run LightGBM with kfold - done in 733s\n",
      "Full model run - done in 733s\n"
     ]
    }
   ],
   "source": [
    "run_lightgbm(full, 100, 'submission_0823_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 600 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02270599, 0.02270599, 0.02264953, 0.02261162, 0.02249934,\n",
       "       0.02242437, 0.02204474, 0.02204119, 0.02202703, 0.0220235 ,\n",
       "       0.02200581, 0.02192771, 0.02170783, 0.02137869, 0.02136823,\n",
       "       0.02133131, 0.02125357, 0.02104046, 0.02086191, 0.02031431,\n",
       "       0.02014387, 0.0200303 , 0.01987966, 0.0195324 , 0.01951811,\n",
       "       0.01927783, 0.01901417, 0.01898446, 0.01896158, 0.01894515])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[np.argsort(-scores)][570:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (307507, 601), test shape: (48744, 600)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's auc: 0.77717\tvalid_1's auc: 0.764326\n",
      "[200]\ttraining's auc: 0.800272\tvalid_1's auc: 0.779249\n",
      "[300]\ttraining's auc: 0.813838\tvalid_1's auc: 0.785343\n",
      "[400]\ttraining's auc: 0.823975\tvalid_1's auc: 0.788696\n",
      "[500]\ttraining's auc: 0.832561\tvalid_1's auc: 0.790557\n",
      "[600]\ttraining's auc: 0.839914\tvalid_1's auc: 0.79166\n",
      "[700]\ttraining's auc: 0.84671\tvalid_1's auc: 0.792362\n",
      "[800]\ttraining's auc: 0.852955\tvalid_1's auc: 0.79277\n",
      "[900]\ttraining's auc: 0.858924\tvalid_1's auc: 0.793034\n",
      "[1000]\ttraining's auc: 0.864293\tvalid_1's auc: 0.793173\n",
      "[1100]\ttraining's auc: 0.869517\tvalid_1's auc: 0.793377\n",
      "[1200]\ttraining's auc: 0.874472\tvalid_1's auc: 0.793533\n",
      "[1300]\ttraining's auc: 0.8794\tvalid_1's auc: 0.793719\n",
      "[1400]\ttraining's auc: 0.883988\tvalid_1's auc: 0.793678\n",
      "[1500]\ttraining's auc: 0.888173\tvalid_1's auc: 0.79375\n",
      "[1600]\ttraining's auc: 0.892327\tvalid_1's auc: 0.793821\n",
      "[1700]\ttraining's auc: 0.896355\tvalid_1's auc: 0.793874\n",
      "[1800]\ttraining's auc: 0.900167\tvalid_1's auc: 0.793864\n",
      "[1900]\ttraining's auc: 0.903713\tvalid_1's auc: 0.793635\n",
      "Early stopping, best iteration is:\n",
      "[1740]\ttraining's auc: 0.897934\tvalid_1's auc: 0.793921\n",
      "Fold  1 AUC : 0.793921\n"
     ]
    }
   ],
   "source": [
    "run_lightgbm(full, 600, 'submission_0823_600.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
