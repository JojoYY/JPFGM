{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johanna/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/johanna/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "\n",
    "# make sure you pip install sklearn_pandas (this is a very useful model)\n",
    "from sklearn_pandas import DataFrameMapper, CategoricalImputer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# UDF\n",
    "import basic_application_data_cleaner as cleaner\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data shape:  (307511, 122)\n",
      "Raw testing data shape:  (48744, 121)\n",
      "Cleaned training data shape:  (307511, 247)\n",
      "Cleaned testing data shape:  (48744, 246)\n"
     ]
    }
   ],
   "source": [
    "path_to_kaggle_data='~/kaggle_JPFGM/Data/'  # location of all the unzipped data files on local machine# Training data\n",
    "app_train, app_test = cleaner.read_raw_application_data(path_to_kaggle_data)\n",
    "df_train, df_test = cleaner.wrangle_application_train_test_data(app_train, app_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, app_data, split=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: sklearn model to be fitted to app_data\n",
    "    app_data: dataframe, subset of the application training data set, containing columns \n",
    "              'SK_ID_CURR' and 'TARGET'\n",
    "    \n",
    "    split: string, default None.\n",
    "           - If None, train model on app_data.\n",
    "           - If 'train_test_split', use sklearn.preprocessing.train_test_split to split app_data into a train and test set\n",
    "    \n",
    "    verbose: booean, default False\n",
    "            whether or not to print comments during model fitting\n",
    "    \"\"\"\n",
    "    df = app_data.copy()\n",
    "    \n",
    "    X = df.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    y = df['TARGET']\n",
    "    \n",
    "    # Create train and test data according to specification\n",
    "    if split == 'train_test_split':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    else:\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "\n",
    "    # Note that X, y, X_train, X_test, y_train, y_test are all dataframes\n",
    "    if verbose:\n",
    "        print('Size of training data split:', X_train.shape)\n",
    "        print('Size of test data split:', X_test.shape)\n",
    "        print('Number of positive labels in training data split:', sum(y_train))\n",
    "        print('Number of positive labels in test data split:', sum(y_test))\n",
    "    \n",
    "    # Fit model on training data\n",
    "    # model needs to have this kind of method and kwargs\n",
    "    model.fit(X_train, y_train, eval_metric = 'auc')\n",
    "    # TOOD: does this create a view to the old \n",
    "\n",
    "    # Evaluate model on training data\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_proba_train = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "    accur_train = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    auc_train = metrics.roc_auc_score(y_train, y_proba_train)\n",
    "    \n",
    "    # TODO: output confusion matrix, more performance metrics, and AUC curve plot        \n",
    "\n",
    "    if verbose:\n",
    "        print('\\nIn sample performance:')\n",
    "        print (\"  Accuracy on train set : %.3f\" % accur_train)\n",
    "        print (\"  AUC on train set: %.3f\" % auc_train)\n",
    "        print('Number of predicted positive labels in training data split:', sum(y_pred_train))\n",
    "    \n",
    "    # Evaluate model on test data if available\n",
    "    if split:\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_proba_test = model.predict_proba(X_test)[:,1]\n",
    "            \n",
    "        accur_test = metrics.accuracy_score(y_test, y_pred_test)\n",
    "        auc_test = metrics.roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "        if verbose:\n",
    "            print('\\nOut of sample performance:')\n",
    "            print (\"  Accuracy on train set: %.3f\" % accur_test)\n",
    "            print (\"  AUC on test set: %.3f\" % auc_test)\n",
    "            print('Number of predicted positive labels in test data split:', sum(y_pred_test))\n",
    "    # Is the input model changed already?\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameter: num_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                           learnig_rate=.1,\n",
    "                           min_child_weight=1,  # because high class imbalance\n",
    "                           max_depth=5,\n",
    "                           gamma=0,\n",
    "                           subsample=0.8,\n",
    "                           colsample_bytree=0.8,\n",
    "                           nthread=4,\n",
    "                           scale_pos_weight=1,   # because high class imbalance\n",
    "                           seed=27)  # num_estimators = 100 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = model.get_xgb_params()\n",
    "xgb_train = xgb.DMatrix(df_train.drop(['SK_ID_CURR', 'TARGET'], axis=1).values,\n",
    "                        df_train['TARGET'].values)\n",
    "\n",
    "cv_result = xgb.cv(xgb_params,\n",
    "                   xgb_train,\n",
    "                   num_boost_round=model.get_params()['n_estimators'], \n",
    "                   nfold=5,\n",
    "                   metrics='auc',\n",
    "                   early_stopping_rounds=50) \n",
    "                  #show_progress=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.699312</td>\n",
       "      <td>0.010719</td>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.011212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712421</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.716961</td>\n",
       "      <td>0.001704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717529</td>\n",
       "      <td>0.004410</td>\n",
       "      <td>0.722780</td>\n",
       "      <td>0.004683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.721037</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.726051</td>\n",
       "      <td>0.004122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.722524</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.728107</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.725357</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.730992</td>\n",
       "      <td>0.001718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.725601</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.731457</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.725414</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.731520</td>\n",
       "      <td>0.002363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.726785</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.732955</td>\n",
       "      <td>0.001664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.726874</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.733250</td>\n",
       "      <td>0.001385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.727231</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.733753</td>\n",
       "      <td>0.001291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.727690</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.734425</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.728307</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.735162</td>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.729196</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.736337</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.729203</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.736481</td>\n",
       "      <td>0.001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.729582</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.736990</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.729991</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.737557</td>\n",
       "      <td>0.001145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.730548</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.738284</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.730765</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.738520</td>\n",
       "      <td>0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.731318</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.739209</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.731464</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.739572</td>\n",
       "      <td>0.001209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.731876</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.740087</td>\n",
       "      <td>0.001416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.732598</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.741222</td>\n",
       "      <td>0.000991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.733715</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.742658</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.734475</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.743645</td>\n",
       "      <td>0.001168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.734737</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.744074</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.735105</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.744605</td>\n",
       "      <td>0.001265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.735692</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.745348</td>\n",
       "      <td>0.001116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.736209</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.746083</td>\n",
       "      <td>0.001173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.736501</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.746847</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.751427</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.771773</td>\n",
       "      <td>0.000618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.751576</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.772192</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.751831</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.772624</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.752011</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.772988</td>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.752186</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.773352</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.752304</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.773691</td>\n",
       "      <td>0.000686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.752403</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.773993</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.752531</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.774334</td>\n",
       "      <td>0.000678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.752660</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.774682</td>\n",
       "      <td>0.000715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.752839</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.775041</td>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.752943</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.775370</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.753039</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.775668</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.753156</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.775996</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.753281</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.776288</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.753426</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.776648</td>\n",
       "      <td>0.000737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.753594</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.776963</td>\n",
       "      <td>0.000691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.753714</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.777335</td>\n",
       "      <td>0.000653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.753773</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.777620</td>\n",
       "      <td>0.000638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.753883</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.777908</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.753936</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.778188</td>\n",
       "      <td>0.000637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.754040</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.778459</td>\n",
       "      <td>0.000646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.754082</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.778750</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.754140</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.778987</td>\n",
       "      <td>0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.754256</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.779336</td>\n",
       "      <td>0.000703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.754310</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.779585</td>\n",
       "      <td>0.000684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.754398</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.779933</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.754495</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.780202</td>\n",
       "      <td>0.000733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.754539</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.780471</td>\n",
       "      <td>0.000727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.754584</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.780771</td>\n",
       "      <td>0.000693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.754679</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.781051</td>\n",
       "      <td>0.000811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-auc-mean  test-auc-std  train-auc-mean  train-auc-std\n",
       "0        0.699312      0.010719        0.704158       0.011212\n",
       "1        0.712421      0.001347        0.716961       0.001704\n",
       "2        0.717529      0.004410        0.722780       0.004683\n",
       "3        0.721037      0.003776        0.726051       0.004122\n",
       "4        0.722524      0.002709        0.728107       0.003322\n",
       "5        0.725357      0.002329        0.730992       0.001718\n",
       "6        0.725601      0.001918        0.731457       0.002400\n",
       "7        0.725414      0.001643        0.731520       0.002363\n",
       "8        0.726785      0.001595        0.732955       0.001664\n",
       "9        0.726874      0.001319        0.733250       0.001385\n",
       "10       0.727231      0.001662        0.733753       0.001291\n",
       "11       0.727690      0.001699        0.734425       0.001832\n",
       "12       0.728307      0.001503        0.735162       0.001472\n",
       "13       0.729196      0.000890        0.736337       0.001119\n",
       "14       0.729203      0.000856        0.736481       0.001061\n",
       "15       0.729582      0.001167        0.736990       0.001220\n",
       "16       0.729991      0.001269        0.737557       0.001145\n",
       "17       0.730548      0.001008        0.738284       0.000976\n",
       "18       0.730765      0.001036        0.738520       0.000932\n",
       "19       0.731318      0.000950        0.739209       0.001238\n",
       "20       0.731464      0.000863        0.739572       0.001209\n",
       "21       0.731876      0.000898        0.740087       0.001416\n",
       "22       0.732598      0.001150        0.741222       0.000991\n",
       "23       0.733715      0.001215        0.742658       0.000796\n",
       "24       0.734475      0.001303        0.743645       0.001168\n",
       "25       0.734737      0.001304        0.744074       0.001111\n",
       "26       0.735105      0.001327        0.744605       0.001265\n",
       "27       0.735692      0.001244        0.745348       0.001116\n",
       "28       0.736209      0.001473        0.746083       0.001173\n",
       "29       0.736501      0.001511        0.746847       0.001153\n",
       "..            ...           ...             ...            ...\n",
       "70       0.751427      0.001600        0.771773       0.000618\n",
       "71       0.751576      0.001584        0.772192       0.000679\n",
       "72       0.751831      0.001595        0.772624       0.000651\n",
       "73       0.752011      0.001606        0.772988       0.000693\n",
       "74       0.752186      0.001575        0.773352       0.000684\n",
       "75       0.752304      0.001582        0.773691       0.000686\n",
       "76       0.752403      0.001561        0.773993       0.000696\n",
       "77       0.752531      0.001597        0.774334       0.000678\n",
       "78       0.752660      0.001595        0.774682       0.000715\n",
       "79       0.752839      0.001492        0.775041       0.000807\n",
       "80       0.752943      0.001459        0.775370       0.000801\n",
       "81       0.753039      0.001436        0.775668       0.000797\n",
       "82       0.753156      0.001512        0.775996       0.000778\n",
       "83       0.753281      0.001534        0.776288       0.000763\n",
       "84       0.753426      0.001536        0.776648       0.000737\n",
       "85       0.753594      0.001525        0.776963       0.000691\n",
       "86       0.753714      0.001567        0.777335       0.000653\n",
       "87       0.753773      0.001546        0.777620       0.000638\n",
       "88       0.753883      0.001557        0.777908       0.000601\n",
       "89       0.753936      0.001545        0.778188       0.000637\n",
       "90       0.754040      0.001556        0.778459       0.000646\n",
       "91       0.754082      0.001585        0.778750       0.000619\n",
       "92       0.754140      0.001615        0.778987       0.000617\n",
       "93       0.754256      0.001588        0.779336       0.000703\n",
       "94       0.754310      0.001585        0.779585       0.000684\n",
       "95       0.754398      0.001579        0.779933       0.000654\n",
       "96       0.754495      0.001532        0.780202       0.000733\n",
       "97       0.754539      0.001541        0.780471       0.000727\n",
       "98       0.754584      0.001504        0.780771       0.000693\n",
       "99       0.754679      0.001467        0.781051       0.000811\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model selection\n",
    "# model.set_params(n_estimators=cv_result.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate OOS performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data split: (206032, 245)\n",
      "Size of test data split: (101479, 245)\n",
      "Number of positive labels in training data split: 16708\n",
      "Number of positive labels in test data split: 8117\n",
      "\n",
      "In sample performance:\n",
      "  Accuracy on train set : 0.920\n",
      "  AUC on train set: 0.785\n",
      "Number of predicted positive labels in training data split: 561\n",
      "\n",
      "Out of sample performance:\n",
      "  Accuracy on train set: 0.920\n",
      "  AUC on test set: 0.757\n",
      "Number of predicted positive labels in test data split: 241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learnig_rate=0.1, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_select = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                           learnig_rate=.1,\n",
    "                           min_child_weight=1,  # because high class imbalance\n",
    "                           max_depth=5,\n",
    "                           gamma=0,\n",
    "                           subsample=0.8,\n",
    "                           colsample_bytree=0.8,\n",
    "                           nthread=4,\n",
    "                           scale_pos_weight=1,   # because high class imbalance\n",
    "                           seed=27)  # num_estimators = 100 by default\n",
    "\n",
    "# print performance using train_test_split\n",
    "fit_model(model_select, df_train, split = 'train_test_split', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions on final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, app_test, filename_output=None):\n",
    "    df_test = app_test.copy()\n",
    "    X_test = df_test.drop(['SK_ID_CURR'], axis=1)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Make DataFrame for submission\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'], 'TARGET': y_pred_test})\n",
    "    \n",
    "    if filename_output is None:\n",
    "        submission.to_csv(filename_output, index = False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default num_estimators = 100\n",
    "model_select = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                           learnig_rate=.1,\n",
    "                           min_child_weight=1,  # because high class imbalance\n",
    "                           max_depth=5,\n",
    "                           gamma=0,\n",
    "                           subsample=0.8,\n",
    "                           colsample_bytree=0.8,\n",
    "                           nthread=4,\n",
    "                           scale_pos_weight=1,   # because high class imbalance\n",
    "                           seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select = fit_model(model_select, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = make_predictions(model_select, df_test, filename_output=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    48682\n",
       "1       62\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submit['TARGET'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
